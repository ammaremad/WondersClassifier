{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19448061",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5291615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNet\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30441f2d",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation and Splitting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ded103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3082 images belonging to 12 classes.\n",
      "Found 764 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training and validation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,            # Normalize images\n",
    "    rotation_range=20,         # Random rotations\n",
    "    width_shift_range=0.2,     # Horizontal shift\n",
    "    height_shift_range=0.2,    # Vertical shift\n",
    "    zoom_range=0.2,            # Random zoom\n",
    "    horizontal_flip=True,      # Flip horizontally\n",
    "    validation_split=0.2       # Split 20% for validation\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    r'D:\\Ammar\\AMIT Diploma\\Machine Leaning\\ML Supervised (challenge)\\archive\\Wonders of World\\Wonders of World',          # Path to your images\n",
    "    target_size=(224, 224),    # Resize images to 224x224\n",
    "    batch_size=32,             # Batch size\n",
    "    class_mode='categorical',  # Categorical labels\n",
    "    subset='training'          # Training data\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    r'D:\\Ammar\\AMIT Diploma\\Machine Leaning\\ML Supervised (challenge)\\archive\\Wonders of World\\Wonders of World',          # Path to your images\n",
    "    target_size=(224, 224),    # Resize images to 224x224\n",
    "    batch_size=32,             # Batch size\n",
    "    class_mode='categorical',  # Categorical labels\n",
    "    subset='validation'        # Validation data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c161884",
   "metadata": {},
   "source": [
    "# 3. Class Weight Calculation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c972ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_data.classes),\n",
    "    y=train_data.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))  # Convert to dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7fe95",
   "metadata": {},
   "source": [
    "# 4. Early Stopping Callback:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f992de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=3,            # Stop after 3 epochs with no improvement\n",
    "    restore_best_weights=True  # Restore the best weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52856af7",
   "metadata": {},
   "source": [
    "# 5. Model 1: VGG16 Transfer Learning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30bc95f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - accuracy: 0.4869 - loss: 1.8360 - val_accuracy: 0.7723 - val_loss: 0.6727\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 3s/step - accuracy: 0.7907 - loss: 0.6706 - val_accuracy: 0.8364 - val_loss: 0.5075\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 3s/step - accuracy: 0.8397 - loss: 0.4778 - val_accuracy: 0.8717 - val_loss: 0.3939\n",
      "Epoch 4/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 3s/step - accuracy: 0.8558 - loss: 0.4425 - val_accuracy: 0.8429 - val_loss: 0.4843\n",
      "Epoch 5/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3s/step - accuracy: 0.8691 - loss: 0.3981 - val_accuracy: 0.8312 - val_loss: 0.6398\n",
      "Epoch 6/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 3s/step - accuracy: 0.8734 - loss: 0.3619 - val_accuracy: 0.8442 - val_loss: 0.5577\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained VGG16 without top layers\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in vgg16_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = Flatten()(vgg16_base.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the VGG16 model\n",
    "vgg16_model = Model(inputs=vgg16_base.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "vgg16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "vgg16_history = vgg16_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee613c3",
   "metadata": {},
   "source": [
    "# 6. Model 2: ResNet50 Transfer Learning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e01e195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1s/step - accuracy: 0.0932 - loss: 5.4074 - val_accuracy: 0.1152 - val_loss: 2.4705\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - accuracy: 0.0931 - loss: 2.4584 - val_accuracy: 0.1021 - val_loss: 2.4839\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 1s/step - accuracy: 0.1090 - loss: 2.4772 - val_accuracy: 0.1034 - val_loss: 2.4857\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet50 without top layers\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = Flatten()(resnet_base.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the ResNet50 model\n",
    "resnet_model = Model(inputs=resnet_base.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "resnet_history = resnet_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dabaa6",
   "metadata": {},
   "source": [
    "# 7. Model 3: MobileNet Transfer Learning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03be436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 670ms/step - accuracy: 0.4976 - loss: 7.8349 - val_accuracy: 0.8442 - val_loss: 0.5305\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 655ms/step - accuracy: 0.8759 - loss: 0.4252 - val_accuracy: 0.8822 - val_loss: 0.4498\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 657ms/step - accuracy: 0.8950 - loss: 0.3398 - val_accuracy: 0.8717 - val_loss: 0.5164\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileNet without top layers\n",
    "mobilenet_base = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in mobilenet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = Flatten()(mobilenet_base.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the MobileNet model\n",
    "mobilenet_model = Model(inputs=mobilenet_base.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "mobilenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "mobilenet_history = mobilenet_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dab777",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5268a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.8609 - loss: 0.4213\n",
      "VGG16 Validation Accuracy: 87.57%\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.1141 - loss: 2.4700\n",
      "ResNet50 Validation Accuracy: 12.30%\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 520ms/step - accuracy: 0.8548 - loss: 0.5372\n",
      "MobileNet Validation Accuracy: 85.47%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate VGG16 model\n",
    "vgg16_scores = vgg16_model.evaluate(val_data)\n",
    "print(f\"VGG16 Validation Accuracy: {vgg16_scores[1] * 100:.2f}%\")\n",
    "\n",
    "# Evaluate ResNet50 model\n",
    "resnet_scores = resnet_model.evaluate(val_data)\n",
    "print(f\"ResNet50 Validation Accuracy: {resnet_scores[1] * 100:.2f}%\")\n",
    "\n",
    "# Evaluate MobileNet model\n",
    "mobilenet_scores = mobilenet_model.evaluate(val_data)\n",
    "print(f\"MobileNet Validation Accuracy: {mobilenet_scores[1] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea1add",
   "metadata": {},
   "source": [
    "# 9. Model Comparison:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46458aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is VGG16 with an accuracy of 87.57%\n"
     ]
    }
   ],
   "source": [
    "# Compare models based on validation accuracy\n",
    "models = {\n",
    "    'VGG16': vgg16_scores[1] * 100,\n",
    "    'ResNet50': resnet_scores[1] * 100,\n",
    "    'MobileNet': mobilenet_scores[1] * 100\n",
    "}\n",
    "\n",
    "best_model = max(models, key=models.get)\n",
    "\n",
    "print(f\"The best model is {best_model} with an accuracy of {models[best_model]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78535346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3846 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create ImageDataGenerator for test data (without augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load test data\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    r'D:\\Ammar\\AMIT Diploma\\Machine Leaning\\ML Supervised (challenge)\\archive\\Wonders of World\\Wonders of World',  # Replace with the actual path\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aac1ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliem\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - accuracy: 0.9281 - loss: 0.2387\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 966ms/step - accuracy: 0.1048 - loss: 2.4686\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 332ms/step - accuracy: 0.8788 - loss: 0.3946\n"
     ]
    }
   ],
   "source": [
    "vgg16_scores = vgg16_model.evaluate(test_data)\n",
    "resnet_scores = resnet_model.evaluate(test_data)\n",
    "mobilenet_scores = mobilenet_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f094b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is `vgg16` model with an accuracy of 0.94%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best model is `vgg16` model with an accuracy of {vgg16_scores[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c1fad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "vgg16_model.save('Wonders_of_the_world_classfication.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e3447",
   "metadata": {},
   "source": [
    "- The best model is `vgg16_model`:\n",
    "- with accuarcy **93%** on the test data \n",
    "- and **87.6%** accuarcy on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1c34b-1bb8-4cae-ad77-01331f54a347",
   "metadata": {},
   "source": [
    "### Project Summary: Classification of Wonders of the World Using Transfer Learning\n",
    "\n",
    "#### Objective:\n",
    "The project aimed to classify images of various wonders of the world using deep learning techniques, specifically leveraging transfer learning with pre-trained models.\n",
    "\n",
    "#### Methodology:\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "   - Utilized TensorFlow and Keras for building and training the models, along with libraries for data preprocessing and handling class imbalance.\n",
    "\n",
    "2. **Data Augmentation and Splitting**:\n",
    "   - Employed `ImageDataGenerator` for data augmentation to enhance the training dataset by applying transformations such as rotation, shifting, zooming, and flipping.\n",
    "   - Split the dataset into training (80%) and validation (20%) sets, ensuring a balanced representation of classes.\n",
    "\n",
    "3. **Class Weight Calculation**:\n",
    "   - Computed class weights to address any class imbalance in the dataset, ensuring that the model pays equal attention to all classes during training.\n",
    "\n",
    "4. **Early Stopping Callback**:\n",
    "   - Implemented early stopping to prevent overfitting by monitoring validation loss and stopping training when no improvement was observed for three consecutive epochs.\n",
    "\n",
    "5. **Model Development**:\n",
    "   - **Model 1: VGG16**:\n",
    "     - Loaded the pre-trained VGG16 model, froze its layers, and added custom classification layers. Achieved a validation accuracy of 87.57%.\n",
    "   - **Model 2: ResNet50**:\n",
    "     - Loaded the ResNet50 model, froze its layers, and added custom layers. However, it performed poorly with a validation accuracy of only 12.30%.\n",
    "   - **Model 3: MobileNet**:\n",
    "     - Loaded the MobileNet model, froze its layers, and added custom layers, achieving a validation accuracy of 85.47%.\n",
    "\n",
    "6. **Model Evaluation**:\n",
    "   - Evaluated all three models on the validation dataset, with VGG16 outperforming the others significantly.\n",
    "   - The best model, VGG16, was further evaluated on a separate test dataset, achieving an impressive accuracy of 93%.\n",
    "\n",
    "7. **Model Saving**:\n",
    "   - The VGG16 model was saved for future use, with a recommendation to use the native Keras format for model saving.\n",
    "\n",
    "#### Results:\n",
    "- The VGG16 model emerged as the best performer with a validation accuracy of 87.57% and a test accuracy of 93%. The ResNet50 model underperformed significantly, while MobileNet showed moderate performance.\n",
    "\n",
    "#### Conclusion:\n",
    "The project successfully demonstrated the effectiveness of transfer learning in image classification tasks, particularly with the VGG16 model. The results indicate that using pre-trained models can significantly enhance classification accuracy, especially in scenarios with limited training data. Future work could explore fine-tuning the models and experimenting with additional architectures to further improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
